{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqH6OBqGdu4A9/2WoKBLrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadywalied/numerical_mahdy/blob/master/ANN_ISO_DATA01_SAVED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE4vLNLeoH1X",
        "outputId": "e4128927-ae21-4fe0-b2bd-40750bec2078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages using pip.\n",
        "# Uncomment: !pip install torch wandb numpy pandas matplotlib\n",
        "\n",
        "# Import required libraries.\n",
        "import torch\n",
        "import pandas as pd  # Import the Pandas library for data manipulation.\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn  # Import PyTorch's neural network module.\n",
        "from torch.autograd import Variable  # Import PyTorch's autograd module for automatic differentiation.\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib for creating plots.\n",
        "%matplotlib inline  # This line is a Jupyter Notebook magic command for inline plotting.\n",
        "\n",
        "\n",
        "# Print the version of PyTorch (incorrect usage of the torch.version module, corrected below).\n",
        "print(torch.__version__)\n",
        "\n",
        "# Note: The following line is commented out because it's a command to clone a GitHub repository, not Python code.\n",
        "# You can uncomment and run it in your terminal or command prompt if needed.\n",
        "# ! git clone https://github.com/hadywalied/numerical_mahdy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # Import the Pandas library for data manipulation.\n",
        "\n",
        "# Define the input file name as a variable for easy modification.\n",
        "input_file = 'output_results.csv'\n",
        "\n",
        "# Read the CSV data from the specified input file into a Pandas DataFrame.\n",
        "csv_data = pd.read_csv(input_file)\n",
        "\n",
        "# Display the first few rows of the DataFrame to inspect the data.\n",
        "csv_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "nVF3LrA5o7U-",
        "outputId": "aa0ec4bb-fac1-41f7-87dc-b8d75af3e53a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-548f6b48a42c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Read the CSV data from the specified input file into a Pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcsv_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Display the first few rows of the DataFrame to inspect the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_results.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, file_name, train_test_ratio=0.9, test=False):\n",
        "        # Read the CSV file into a Pandas DataFrame.\n",
        "        _df = pd.read_csv(file_name)\n",
        "\n",
        "        if test:\n",
        "            # Calculate the length of data for the test set.\n",
        "            data_len = math.floor((1 - train_test_ratio) * len(_df.iloc[:, 0]))\n",
        "        else:\n",
        "            # Calculate the length of data for the training set.\n",
        "            data_len = math.floor(train_test_ratio * len(_df.iloc[:, 0]))\n",
        "\n",
        "        # Extract input features (x) and target values (y) from the DataFrame.\n",
        "        x = _df.iloc[:data_len, 1:-2].values\n",
        "        y = _df.iloc[:data_len, -2:].values\n",
        "\n",
        "        # Convert the NumPy arrays to PyTorch tensors with float32 data type.\n",
        "        self.x_data = torch.tensor(x, dtype=torch.float32)\n",
        "        self.y_data = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of samples in the dataset (length of y_data).\n",
        "        return np.shape(self.y_data)[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a tuple containing the input data (x_data) and target data (y_data) for a given index (idx).\n",
        "        return self.x_data[idx], self.y_data[idx]"
      ],
      "metadata": {
        "id": "uZEGVySDpEPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a training dataset instance using the MyDataset class and the input_file.\n",
        "train_dataset = MyDataset(input_file)\n",
        "\n",
        "# Create a testing dataset instance using the MyDataset class, specifying 'test=True'.\n",
        "test_dataset = MyDataset(input_file, test=True)"
      ],
      "metadata": {
        "id": "Aj6lRuXxpP71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Model Architecture"
      ],
      "metadata": {
        "id": "lnfp8zLppdAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OX6HGlaIpzhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''torch.manual_seed(1): This function call sets the random seed of the PyTorch library to 1.\n",
        "When random numbers are generated using PyTorch functions, they will be generated in a deterministic manner, always producing the same sequence of random numbers if the random seed is not changed.\n",
        "This is useful for reproducibility in machine learning experiments.'''\n",
        "\n",
        "torch.manual_seed(1) # Reproducible"
      ],
      "metadata": {
        "id": "xPOIc26Rpy2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''EPOCH = No. if EPOCHs: This constant defines the number of training epochs. An epoch is one complete pass through the entire training dataset during the training process.\n",
        "In this case, the model will be trained for 100 epochs, meaning it will see the entire training dataset 100 times during training.'''\n",
        "\n",
        "'''BATCH_SIZE = 37: This constant defines the batch size used in mini-batch gradient descent. During training, the dataset is divided into smaller batches, each containing 37 data samples.\n",
        "The model's parameters are updated after processing each batch. Using mini-batches can speed up training and make it more memory-efficient compared to processing the entire dataset at once.'''\n",
        "\n",
        "EPOCH = 100\n",
        "BATCH_SIZE = 37\n",
        "LR = 0.001"
      ],
      "metadata": {
        "id": "ns-D9lKYpZ9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for the training dataset.\n",
        "# - train_dataset: The dataset containing training samples.\n",
        "# - batch_size: The batch size for mini-batch training.\n",
        "# - shuffle: Whether to shuffle the dataset before each epoch (False in this case).\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Create a DataLoader for the test dataset (similar to the training DataLoader).\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "YKg5DSTyq5mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the next batch of training data and labels using the DataLoader.\n",
        "# `iter(train_loader)` creates an iterator for the DataLoader, and `next(iter(...))` gets the next batch.\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "\n",
        "# Print the shape (dimensions) of the batch of training features.\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "\n",
        "# Print the shape (dimensions) of the batch of training labels.\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n"
      ],
      "metadata": {
        "id": "XKjq3EYjrYbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below defines a neural network model for multiple linear regression. It has an input layer with 6 features, two hidden layers with 6 neurons each, and an output layer with the specified output dimension. The Xavier initialization method is used to initialize the weights, and the forward method defines the forward pass of the network."
      ],
      "metadata": {
        "id": "CawQqU5nr_nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultipleLinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim=6, output_dim=2):\n",
        "        super(MultipleLinearRegression, self).__init__()\n",
        "        # Define the layers of the neural network.\n",
        "        self.hidden1 = nn.Linear(input_dim, 6)  # First hidden layer with 6 neurons.\n",
        "        self.hidden2 = nn.Linear(6, 6)           # Second hidden layer with 6 neurons.\n",
        "        self.output = nn.Linear(6, output_dim)   # Output layer with the specified output dimension.\n",
        "\n",
        "        # Initialize the weights and biases using Xavier initialization.\n",
        "        nn.init.xavier_uniform_(self.hidden1.weight)\n",
        "        nn.init.zeros_(self.hidden1.bias)\n",
        "        nn.init.xavier_uniform_(self.hidden2.weight)\n",
        "        nn.init.zeros_(self.hidden2.bias)\n",
        "        nn.init.xavier_uniform_(self.output.weight)\n",
        "        nn.init.zeros_(self.output.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of the neural network.\n",
        "        z = torch.sigmoid(self.hidden1(x))  # Apply sigmoid activation to the first hidden layer.\n",
        "        z = torch.sigmoid(self.hidden2(z))  # Apply sigmoid activation to the second hidden layer.\n",
        "        z = self.output(z)                   # Output layer with no activation function.\n",
        "        return z\n"
      ],
      "metadata": {
        "id": "nxHCOm_Hr52p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLR_model = MultipleLinearRegression(6,2)\n",
        "print(\"The parameters: \", list(MLR_model.parameters()))"
      ],
      "metadata": {
        "id": "Ds9Zfa56saWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an AdamW optimizer for model parameter optimization with specified learning rate (LR)\n",
        "optimizer = torch.optim.AdamW(MLR_model.parameters(), lr=LR)\n",
        "\n",
        "# Define the Mean Squared Error (MSE) loss criterion for regression tasks\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "1kc4KA49sa9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input data as a tensor (make sure the values are floats)\n",
        "x = torch.tensor([8.0, 6.0, 10.0, 1.0, 16.0, 100.0], dtype=torch.float32)\n",
        "\n",
        "# Use the model to make predictions (NO TRAINING YET)\n",
        "y_pred = MLR_model(x)\n",
        "\n",
        "# Print the predicted values\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "uXpbcsH6suZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "tZUuawZ7tS5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oZL7n6n7pvXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store training losses\n",
        "t_losses = []\n",
        "\n",
        "# Loop over the specified number of epochs\n",
        "for epoch in range(EPOCH):\n",
        "    # Loop over batches of training data\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        # Zero the gradients to avoid accumulation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute model predictions\n",
        "        outputs = MLR_model(inputs)\n",
        "\n",
        "        # Calculate the loss between model predictions and actual targets\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Append the current loss value to the list of training losses\n",
        "        t_losses.append(loss.item())\n",
        "\n",
        "        # Backward pass: compute gradients of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model's parameters using the optimizer\n",
        "        optimizer.step()\n",
        "        # scheduler.step()  # Optionally adjust the learning rate\n",
        "\n",
        "        # Print training statistics every 10 batches\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{EPOCH}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "I0ynheOMtXmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "tt0PcsUnuA1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use torch.no_grad() to temporarily disable gradient tracking to save memory\n",
        "with torch.no_grad():\n",
        "    # Initialize a variable to store the total test loss\n",
        "    total_loss = 0\n",
        "\n",
        "    # Loop over batches of test data from test_loader\n",
        "    for inputs, targets in test_loader:\n",
        "        # Compute model predictions for the current batch\n",
        "        outputs = MLR_model(inputs)\n",
        "\n",
        "        # Calculate the loss between model predictions and actual targets\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Add the current loss to the total test loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the mean test loss over all test batches\n",
        "    mean_loss = total_loss / len(test_loader)\n",
        "\n",
        "    # Print the mean test loss\n",
        "    print(f'Test Loss: {mean_loss:.4f}')"
      ],
      "metadata": {
        "id": "VVzZ-saEt9Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "I-uhBf_cuSX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training losses stored in t_losses\n",
        "plt.plot(t_losses)  # Plotting the list of training losses\n",
        "\n",
        "# Add labels to the x and y axes\n",
        "plt.xlabel(\"Number of Iterations\")  # Label for the x-axis\n",
        "plt.ylabel(\"Total Loss\")  # Label for the y-axis\n",
        "\n",
        "# Display the plot\n",
        "plt.show()  # Show the generated plot"
      ],
      "metadata": {
        "id": "ib02uy7ouQks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Validation"
      ],
      "metadata": {
        "id": "cbGj4mCvug_0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cg2-Pz5yugMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}